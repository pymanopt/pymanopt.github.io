<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Riemannian Optimization for Inference in MoG models &mdash; Pymanopt latest (2.2.2.dev1+ga1f52e7) documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/style.css" type="text/css" />
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
    <link rel="canonical" href="pymanopt.org/examples/notebooks/mixture_of_gaussians.html" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"></script>
        <script src="../../_static/katex_autorenderer.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Contributing" href="../../CONTRIBUTING.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Pymanopt
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                latest (2.2.2.dev1+ga1f52e7)
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/pymanopt/pymanopt/tree/master/examples">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api-reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../CONTRIBUTING.html">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notebooks</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Riemannian Optimization for Inference in MoG models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#When-Things-Go-Astray">When Things Go Astray</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Pymanopt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Riemannian Optimization for Inference in MoG models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/examples/notebooks/mixture_of_gaussians.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="admonition note">
<p><span class="raw-html"><a href="https://github.com/pymanopt/pymanopt/blob/master/examples/notebooks/mixture_of_gaussians.ipynb"><img alt="Open on GitHub"
src="https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub"
style="vertical-align:text-bottom"></a></span></p>
</div>
<div class="section" id="Riemannian-Optimization-for-Inference-in-MoG-models">
<h1>Riemannian Optimization for Inference in MoG models<a class="headerlink" href="#Riemannian-Optimization-for-Inference-in-MoG-models" title="Permalink to this headline"></a></h1>
<p>The Mixture of Gaussians (MoG) model assumes that datapoints <span class="math">\(\mathbf{x}_i\in\mathbb{R}^d\)</span> follow a distribution described by the following probability density function:</p>
<p><span class="math">\(p(\mathbf{x}) = \sum_{m=1}^M \pi_m p_\mathcal{N}(\mathbf{x};\mathbf{\mu}_m,\mathbf{\Sigma}_m)\)</span> where <span class="math">\(\pi_m\)</span> is the probability that the data point belongs to the <span class="math">\(m^\text{th}\)</span> mixture component and <span class="math">\(p_\mathcal{N}(\mathbf{x};\mathbf{\mu}_m,\mathbf{\Sigma}_m)\)</span> is the probability density function of a multivariate Gaussian distribution with mean <span class="math">\(\mathbf{\mu}_m \in \mathbb{R}^d\)</span> and psd covariance matrix
<span class="math">\(\mathbf{\Sigma}_m \in \{\mathbf{M}\in\mathbb{R}^{d\times d}: \mathbf{M}\succeq 0\}\)</span>.</p>
<p>As an example consider the mixture of three Gaussians with means <span class="math">\(\mathbf{\mu}_1 = \begin{bmatrix} -4 \\ 1 \end{bmatrix}\)</span>, <span class="math">\(\mathbf{\mu}_2 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span> and <span class="math">\(\mathbf{\mu}_3 = \begin{bmatrix} 2 \\ -1 \end{bmatrix}\)</span>, covariances <span class="math">\(\mathbf{\Sigma}_1 = \begin{bmatrix} 3 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span>, <span class="math">\(\mathbf{\Sigma}_2 = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 3 \end{bmatrix}\)</span> and <span class="math">\(\mathbf{\Sigma}_3 = \begin{bmatrix} 0.5 &amp; 0 \\ 0 &amp; 0.5 \end{bmatrix}\)</span>
and mixture probability vector <span class="math">\(\boldsymbol{\pi}=\left[0.1, 0.6, 0.3\right]^\top\)</span>. Let’s generate <span class="math">\(N=1000\)</span> samples of that MoG model and scatter plot the samples:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="c1"># Number of data points</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Dimension of each data point</span>
<span class="n">D</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Number of clusters</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">pi</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">Sigma</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]),</span>
    <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pi</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>
<span class="c1"># For each component, generate all needed samples</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="c1"># indices of current component in X</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">k</span> <span class="o">==</span> <span class="n">components</span>
    <span class="c1"># number of those occurrences</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">n_k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">samples</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">Sigma</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">n_k</span>
        <span class="p">)</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">k</span> <span class="o">==</span> <span class="n">components</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">samples</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">samples</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">k</span> <span class="o">%</span> <span class="n">K</span><span class="p">],</span>
    <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/examples_notebooks_mixture_of_gaussians_2_0.svg" src="../../_images/examples_notebooks_mixture_of_gaussians_2_0.svg" /></div>
</div>
<p>Given a data sample the de facto standard method to infer the parameters is the <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm">expectation maximisation</a> (EM) algorithm that, in alternating so-called E and M steps, maximises the log-likelihood of the data. In <a class="reference external" href="http://arxiv.org/pdf/1506.07677v1.pdf">arXiv:1506.07677</a> Hosseini and Sra propose Riemannian optimisation as a powerful counterpart to EM. Importantly, they introduce a reparameterisation that leaves local optima
of the log-likelihood unchanged while resulting in a geodesically convex optimisation problem over a product manifold <span class="math">\(\prod_{m=1}^M\mathcal{PD}^{(d+1)\times(d+1)}\)</span> of manifolds of <span class="math">\((d+1)\times(d+1)\)</span> symmetric positive definite matrices. The proposed method is on par with EM and shows less variability in running times.</p>
<p>The reparameterised optimisation problem for augmented data points <span class="math">\(\mathbf{y}_i=[\mathbf{x}_i^\top, 1]^\top\)</span> can be stated as follows:</p>
<div class="math">
\[\min_{(\mathbf{S}_1, ..., \mathbf{S}_m, \boldsymbol{\nu}) \in \mathcal{D}}
-\sum_{n=1}^N\log\left(
\sum_{m=1}^M \frac{\exp(\nu_m)}{\sum_{k=1}^M\exp(\nu_k)}
q_\mathcal{N}(\mathbf{y}_n;\mathbf{S}_m)
\right)\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math">\(\mathcal{D} := \left(\prod_{m=1}^M \mathcal{PD}^{(d+1)\times(d+1)}\right)\times\mathbb{R}^{M-1}\)</span> is the search space</p></li>
<li><p><span class="math">\(\mathcal{PD}^{(d+1)\times(d+1)}\)</span> is the manifold of symmetric positive definite <span class="math">\((d+1)\times(d+1)\)</span> matrices</p></li>
<li><p><span class="math">\(\mathcal{\nu}_m = \log\left(\frac{\alpha_m}{\alpha_M}\right), \ m=1, ..., M-1\)</span> and <span class="math">\(\nu_M=0\)</span></p></li>
<li><p><span class="math">\(q_\mathcal{N}(\mathbf{y}_n;\mathbf{S}_m) = 2\pi\exp\left(\frac{1}{2}\right) |\operatorname{det}(\mathbf{S}_m)|^{-\frac{1}{2}}(2\pi)^{-\frac{d+1}{2}} \exp\left(-\frac{1}{2}\mathbf{y}_i^\top\mathbf{S}_m^{-1}\mathbf{y}_i\right)\)</span></p></li>
</ul>
<p><strong>Optimisation problems like this can easily be solved using Pymanopt – even without the need to differentiate the cost function manually!</strong></p>
<p>So let’s infer the parameters of our toy example by Riemannian optimisation using Pymanopt:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">sys</span>


<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;../..&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">autograd.scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span>

<span class="kn">import</span> <span class="nn">pymanopt</span>
<span class="kn">from</span> <span class="nn">pymanopt</span> <span class="kn">import</span> <span class="n">Problem</span>
<span class="kn">from</span> <span class="nn">pymanopt.manifolds</span> <span class="kn">import</span> <span class="n">Euclidean</span><span class="p">,</span> <span class="n">Product</span><span class="p">,</span> <span class="n">SymmetricPositiveDefinite</span>
<span class="kn">from</span> <span class="nn">pymanopt.optimizers</span> <span class="kn">import</span> <span class="n">SteepestDescent</span>


<span class="c1"># (1) Instantiate the manifold</span>
<span class="n">manifold</span> <span class="o">=</span> <span class="n">Product</span><span class="p">([</span><span class="n">SymmetricPositiveDefinite</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">K</span><span class="p">),</span> <span class="n">Euclidean</span><span class="p">(</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>

<span class="c1"># (2) Define cost function</span>
<span class="c1"># The parameters must be contained in a list theta.</span>
<span class="nd">@pymanopt</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">autograd</span><span class="p">(</span><span class="n">manifold</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># Unpack parameters</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">logdetS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">slogdet</span><span class="p">(</span><span class="n">S</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">samples</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Calculate log_q</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># &#39;Probability&#39; of y belonging to each cluster</span>
    <span class="n">log_q</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">logdetS</span><span class="p">)</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">loglikvec</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loglikvec</span><span class="p">)</span>


<span class="n">problem</span> <span class="o">=</span> <span class="n">Problem</span><span class="p">(</span><span class="n">manifold</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>

<span class="c1"># (3) Instantiate a Pymanopt optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SteepestDescent</span><span class="p">(</span><span class="n">verbosity</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># let Pymanopt do the rest</span>
<span class="n">Xopt</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">problem</span><span class="p">)</span><span class="o">.</span><span class="n">point</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024-11-15 10:08:26.097910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Optimizing...
Terminated - min step_size reached after 226 iterations, 0.73 seconds.

</pre></div></div>
</div>
<p>Once Pymanopt has finished the optimisation we can obtain the inferred parameters as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">mu1hat</span> <span class="o">=</span> <span class="n">Xopt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">Sigma1hat</span> <span class="o">=</span> <span class="n">Xopt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu1hat</span> <span class="o">@</span> <span class="n">mu1hat</span><span class="o">.</span><span class="n">T</span>
<span class="n">mu2hat</span> <span class="o">=</span> <span class="n">Xopt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">Sigma2hat</span> <span class="o">=</span> <span class="n">Xopt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu2hat</span> <span class="o">@</span> <span class="n">mu2hat</span><span class="o">.</span><span class="n">T</span>
<span class="n">mu3hat</span> <span class="o">=</span> <span class="n">Xopt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">Sigma3hat</span> <span class="o">=</span> <span class="n">Xopt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">][:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu3hat</span> <span class="o">@</span> <span class="n">mu3hat</span><span class="o">.</span><span class="n">T</span>
<span class="n">pihat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">Xopt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">pihat</span> <span class="o">=</span> <span class="n">pihat</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pihat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>And convince ourselves that the inferred parameters are close to the ground truth parameters.</p>
<p>The ground truth parameters <span class="math">\(\mathbf{\mu}_1, \mathbf{\Sigma}_1, \mathbf{\mu}_2, \mathbf{\Sigma}_2, \mathbf{\mu}_3, \mathbf{\Sigma}_3, \pi_1, \pi_2, \pi_3\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Sigma</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[-4  1]
[[3 0]
 [0 1]]
[0 0]
[[1. 1.]
 [1. 3.]]
[ 2 -1]
[[0.5 0. ]
 [0.  0.5]]
0.1
0.6
0.3
</pre></div></div>
</div>
<p>And the inferred parameters <span class="math">\(\hat{\mathbf{\mu}}_1, \hat{\mathbf{\Sigma}}_1, \hat{\mathbf{\mu}}_2, \hat{\mathbf{\Sigma}}_2, \hat{\mathbf{\mu}}_3, \hat{\mathbf{\Sigma}}_3, \hat{\pi}_1, \hat{\pi}_2, \hat{\pi}_3\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">mu1hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Sigma1hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mu2hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Sigma2hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mu3hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Sigma3hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pihat</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pihat</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pihat</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[0.11]
 [0.01]]
[[0.97 0.91]
 [0.91 3.1 ]]
[[-3.73]
 [ 0.74]]
[[ 4.28 -0.47]
 [-0.47  0.96]]
[[ 2.06]
 [-1.1 ]]
[[0.44 0.08]
 [0.08 0.48]]
0.610255916888142
0.11620618592965562
0.2735378971822024
</pre></div></div>
</div>
<p>Et voilà – this was a brief demonstration of how to do inference for MoG models by performing Manifold optimisation using Pymanopt.</p>
<div class="section" id="When-Things-Go-Astray">
<h2>When Things Go Astray<a class="headerlink" href="#When-Things-Go-Astray" title="Permalink to this headline"></a></h2>
<p>A well-known problem when fitting parameters of a MoG model is that one Gaussian may collapse onto a single data point resulting in singular covariance matrices (cf. e.g. p. 434 in Bishop, C. M. “Pattern Recognition and Machine Learning.” 2001). This problem can be avoided by the following heuristic: if a component’s covariance matrix is close to being singular we reset its mean and covariance matrix. Using Pymanopt this can be accomplished by using an appropriate line search rule (based on
<a class="reference external" href="https://github.com/pymanopt/pymanopt/blob/master/pymanopt/optimizers/line_search.py">BackTrackingLineSearcher</a>) – here we demonstrate this approach:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">LineSearchMoG</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Back-tracking line-search that checks for close to singular matrices.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">contraction_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">optimism</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">sufficient_decrease</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">max_iterations</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
        <span class="n">initial_step_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">contraction_factor</span> <span class="o">=</span> <span class="n">contraction_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimism</span> <span class="o">=</span> <span class="n">optimism</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sufficient_decrease</span> <span class="o">=</span> <span class="n">sufficient_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iterations</span> <span class="o">=</span> <span class="n">max_iterations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initial_step_size</span> <span class="o">=</span> <span class="n">initial_step_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_oldf0</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">manifold</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">f0</span><span class="p">,</span> <span class="n">df0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to perform backtracking line-search.</span>
<span class="sd">        Arguments:</span>
<span class="sd">            - objective</span>
<span class="sd">                objective function to optimise</span>
<span class="sd">            - manifold</span>
<span class="sd">                manifold to optimise over</span>
<span class="sd">            - x</span>
<span class="sd">                starting point on the manifold</span>
<span class="sd">            - d</span>
<span class="sd">                tangent vector at x (descent direction)</span>
<span class="sd">            - df0</span>
<span class="sd">                directional derivative at x along d</span>
<span class="sd">        Returns:</span>
<span class="sd">            - step_size</span>
<span class="sd">                norm of the vector retracted to reach newx from x</span>
<span class="sd">            - newx</span>
<span class="sd">                next iterate suggested by the line-search</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compute the norm of the search direction</span>
        <span class="n">norm_d</span> <span class="o">=</span> <span class="n">manifold</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_oldf0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Pick initial step size based on where we were last time.</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">f0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_oldf0</span><span class="p">)</span> <span class="o">/</span> <span class="n">df0</span>
            <span class="c1"># Look a little further</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimism</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_step_size</span> <span class="o">/</span> <span class="n">norm_d</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>

        <span class="c1"># Make the chosen step and compute the cost there.</span>
        <span class="n">newx</span><span class="p">,</span> <span class="n">newf</span><span class="p">,</span> <span class="n">reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_newxnewf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">manifold</span><span class="p">)</span>
        <span class="n">step_count</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Backtrack while the Armijo criterion is not satisfied</span>
        <span class="k">while</span> <span class="p">(</span>
            <span class="n">newf</span> <span class="o">&gt;</span> <span class="n">f0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sufficient_decrease</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">df0</span>
            <span class="ow">and</span> <span class="n">step_count</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iterations</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">reset</span>
        <span class="p">):</span>

            <span class="c1"># Reduce the step size</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">contraction_factor</span> <span class="o">*</span> <span class="n">alpha</span>

            <span class="c1"># and look closer down the line</span>
            <span class="n">newx</span><span class="p">,</span> <span class="n">newf</span><span class="p">,</span> <span class="n">reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_newxnewf</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">manifold</span>
            <span class="p">)</span>

            <span class="n">step_count</span> <span class="o">=</span> <span class="n">step_count</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># If we got here without obtaining a decrease, we reject the step.</span>
        <span class="k">if</span> <span class="n">newf</span> <span class="o">&gt;</span> <span class="n">f0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">reset</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">newx</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">step_size</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">norm_d</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_oldf0</span> <span class="o">=</span> <span class="n">f0</span>

        <span class="k">return</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">newx</span>

    <span class="k">def</span> <span class="nf">_newxnewf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">manifold</span><span class="p">):</span>
        <span class="n">newx</span> <span class="o">=</span> <span class="n">manifold</span><span class="o">.</span><span class="n">retraction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">newf</span> <span class="o">=</span> <span class="n">objective</span><span class="p">(</span><span class="n">newx</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="n">replace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">newx</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">k</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
                    <span class="o">!=</span> <span class="n">newx</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">newx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="p">]</span>
            <span class="p">)</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">replace</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">manifold</span><span class="o">.</span><span class="n">random_point</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="n">replace</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">newx</span><span class="p">,</span> <span class="n">newf</span><span class="p">,</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../CONTRIBUTING.html" class="btn btn-neutral float-left" title="Contributing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016-2024, Jamie Townsend, Niklas Koep, Sebastian Weichwald.
      <span class="lastupdated">Last updated on Nov 15, 2024.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: latest
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd><a href="/docs/stable">stable</a></dd>
      <dd><a href="/docs/latest">latest</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>