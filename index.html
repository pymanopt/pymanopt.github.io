<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.1//EN" "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-2.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" dir="ltr" id="index">
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Pymanopt</title>

    <!-- mathjax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    <!-- code-prettify -->
    <script type="text/javascript" src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?lang=py"></script>

    <link href="layout.css" rel="stylesheet" type="text/css" />
</head>

<body>

<h1>Pymanopt</h1>

<p>Pymanopt is a Python package for doing manifold optimization, that computes
gradients and hessians automatically. It builds upon the MATLAB toolbox
<a href="http://manopt.org/">Manopt</a> but is otherwise independent of it.
Pymanopt aims to lower the barriers for users wishing to use state of the art
manifold optimization techniques, by relying on automatic differentiation for
computing gradients and hessians, saving users time and saving them from
potential calculation and implementiation errors.</p>

<p>Pymanopt is modular and hence easy to use. All of the automatic
differentiation is done behind the scenes, so that the amount of setup the user
needs to do is minimal. Usually only the following steps are required:</p>
<ol>
    <li>Defininition of a cost function $f:\mathcal{M}\to \mathbb{R}$ to
      minimise</li>
    <li>Instantiation of a manifold $\mathcal{M}$ to optimise over</li>
    <li>Instantiation of a Pymanopt solver</li>
</ol>

<p>Experimenting with manifold optimisation is simple with Pymanopt. The
following example demonstrates this. The steps will be discussed in more detail
in the subsequent three sections.</p>

<pre class="prettyprint"><code class="language-py">
import autograd.numpy as np

from pymanopt import Problem
from pymanopt.solvers import SteepestDescent
from pymanopt.manifolds import Stiefel

# (1) Define the cost function (here using autograd.numpy)
def cost(X): return np.sum(X)

# (2) Instantiate a manifold
manifold = Stiefel(5, 2)

# (3) Instantiate a Pymanopt solver
solver = SteepestDescent()

# let Pymanopt do the rest
problem = Problem(manifold=manifold, cost=cost)
Xopt = solver.solve(problem)
print(Xopt)
</code></pre>

<p><strong>We encourage users to report problems, request features, ask for
help, or leave general comments either on
<a href="https://github.com/pymanopt/pymanopt">github</a>,
<a href="https://gitter.im/pymanopt/pymanopt">gitter</a>, or via email to
one of the maintainers.</strong></p>


<h2>0. Installing Pymanopt</h2>
<h3>Dependencies</h3>
<p>Pymanopt is compatible with Python 2.7 and Python 3.3+, and depends on
NumPy and SciPy. Additionally, to use Pymanopt's built-in automatic
differentiation, which we strongly recommend, you need to setup your cost
functions using either
<a href="http://www.deeplearning.net/software/theano/">Theano</a> or
<a href="https://github.com/HIPS/autograd">Autograd</a>. If you're unfamiliar
with both packages and not sure which to go for, it's probably best to start
with Autograd. Autograd wraps thinly around NumPy, and is very simple to use,
particularly if you're already familiar with NumPy (see
<a href="#autograd-example">below</a>).</p>

<p>Instructions for installing NumPy, SciPy, and Theano on different operating
systems can be found
<a href="http://deeplearning.net/software/theano/install.html">here</a>,
for installing Autograd
<a href="https://github.com/HIPS/autograd#how-to-install">here</a>.</p>

<h3>Pymanopt installation</h3>
<p>Pymanopt can be installed with the following command:</p>
<pre class="prettyprint"><code class="language-bash">
pip install --user git+https://github.com/pymanopt/pymanopt.git
</code></pre>


<h2>1. Cost Functions</h2>

<p>The cost function passed to Pymanopt should take a single input (a point on
the manifold), and return a scalar. You have three options for how to define
the cost function:</p>
<table id="cost-setup">
  <tr>
    <th></th><th>Method</th><th>Extra work required</th>
  </tr>
  <tr>
    <td>(a)</td><td>Use Autograd</td><td>None</td>
  </tr>
  <tr>
    <td>(b)</td><td>Use Theano</td><td>None</td>
  </tr>
  <tr>
    <td>(c)</td><td>Use a regular Python function</td><td>Calculate and implement
    derivatives (gradient and hessian) by hand.</td>
  </tr>
</table>

<p>The first two options are recommended â€“ indeed, they are what makes
manifold optimization with Pymanopt so easy!</p>

<h3>(a/b) Use Autograd or Theano</h3>

<p>Currently Pymanopt supports Theano and Autograd as autodiff backends. We
plan to implement support for
<a href="https://www.tensorflow.org">TensorFlow</a>.</p>

<h4 id="autograd-example">Setup the cost function using Autograd</h4>
<p>If you already know how to use NumPy, then this approach will be easy. Just
import autograd.numpy and setup your cost as a Python function, using the
autograd numpy to perform the computation.</p>
<pre class="prettyprint"><code class="language-python">
import autograd.numpy as np

def cost(X):
    return np.exp(-np.sum(X**2))
</code></pre>


<h4>Setup the cost function using Theano</h4>
<p>This approach requires you to setup your cost as a Theano graph. A
tutorial on using Theano can be found
<a href="http://deeplearning.net/software/theano/tutorial/">here</a>.</p>
<pre class="prettyprint"><code class="language-python">
import theano.tensor as T

X = T.matrix()
cost = T.exp(-T.sum(X**2))
</code></pre>


<h3>(c) Use a regular Python function</h3>

<p>If you wish to use one of the derivative free solvers (perhaps your cost
function is nowhere smooth), then this approach makes sense. If you want to use
a solver which requires derivatives (these solvers generally perform far better
than derivative free methods) this approach enables you to calculate and
implement gradients and hessians by hand.</p>

<p>Using Pymanopt in this way is similar to Manopt. You can refer to the Manopt
<a href="http://www.manopt.org/tutorial.html">tutorial</a> and
<a href="https://groups.google.com/forum/#!forum/manopttoolbox">forum</a> for
advice on calculating gradients/hessians by hand.
However, we would like to stress that there is <i>little or no advantage</i> to
doing things in this way. The gradients and hessians calculated by Pymanopt
should be both correct and efficient.
</p>
<pre class="prettyprint"><code class="language-python">
problem = Problem(manifold=manifold, cost=cost, egrad=egrad, ehess=ehess)
</code></pre>

<h2>2. Manifolds</h2>

<p>The Pymanopt Manifold class provides manifold specific routines like
computing the intrinsic mean of two points on the manifold or computing
the geodesic distance. The user will only need to instantiate the
correct manifold and need not worry about the internal workings. We plan
on implementing further manifolds as needed by the users. Developers
wanting to implement a new manifold for Pymanopt are referred to the
<a href="doc/index.html#module-pymanopt.manifolds.manifold">Manifold
abstract base class</a>.</p>

<h3>List of Manifolds</h3>

<table id="manifolds">

<tr>
    <th>manifold</th>
    <th>description</th>
    <th>required codelines</th>
</tr>

<tr>
    <td>Euclidean manifold<br />
    $\mathbb{R}^{m\times n}$</td>
    <td>Euclidean space of $m\times n$ matrices equipped with the
    Frobenius distance and trace inner product. Helpful for
    unconstrained problems or, when used as part of a product manifold,
    for unconstrained hyperparameters.</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.manifolds import Euclidean
manifold = Euclidean(m,n)
</code></pre>
    </td>
</tr>

<tr>
    <td>Grassmann manifold<br />
    $G_{m,n} \triangleq \{\operatorname{Span}\{M\}:M\in\mathbb{R}^{m\times n}\}$</td>
    <td>This is the manifold of $n$-dimensional subspaces of $\mathbb{R}^m$. For optional argument $k>1$ this instantiates the product $\mathcal{Gr}^{m\times n}\times\cdots\times\mathcal{Gr}^{m\times n}$ of $k$ Grassmann manifolds.</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.manifolds import Grassmann
manifold = Grassmann(m,n)
</code></pre>
    </td>
</tr>

<tr>
    <td>Oblique manifold<br />
    $O_{m,n} \triangleq \{M\in\mathbb{R}^{m\times n}: ||M_{:,j}||_2=1 \ \forall j\in\mathbb{N}_1^n\}$</td>
    <td>Manifold of matrices with unit-norm columns.</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.manifolds import Oblique
manifold = Oblique(m,n)
</code></pre>
    </td>
</tr>

<tr>
    <td>Elliptope manifold<br />
    $E_k^n \triangleq \{M\in\mathbb{R}^{n\times n}: M \succeq \mathbf{0}, \operatorname{rank}(M)=k, ||M_{i,:}||_2=1 \ \forall i\in\mathbb{N}_1^n\}$</td>
    <td>Manifold of $n$-by-$n$ psd matrices of rank $k$ with unit diagonal elements.</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.manifolds import Elliptope
manifold = Elliptope(n,k)
</code></pre>
    </td>
</tr>

<tr>
    <td>PSDFixedRank manifold<br />
    $\operatorname{PSD}_k^n \triangleq \{M\in\mathbb{R}^{n\times n}: M \succeq \mathbf{0}, \operatorname{rank}(M)=k\}$</td>
    <td>Manifold of $n$-by-$n$ symmetric/hermitian positive semidefinite matrices of rank $k$.</td>
    <td>
<pre class="prettyprint"><code class="language-py">
# real
from pymanopt.manifolds import PSDFixedRank
manifold = PSDFixedRank(n,k)

# complex
from pymanopt.manifolds import PSDFixedRankComplex
manifold = PSDFixedRankComplex(n,k)
</code></pre>
    </td>
</tr>

<tr>
    <td>Sphere manifold<br />
    $\mathcal{S}^{m\times n} \triangleq \{M\in\mathbb{R}^{m\times n}:||M||_F=1\}$</td>
    <td>$m\times n$ matrices of unit Frobenius norm, i.e., the unit
    Sphere for $n=1$ (the default value of this optional argument).</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.manifolds import Sphere
manifold = Sphere(m)
</code></pre>
    </td>
</tr>

<tr>
    <td>Stiefel manifold<br />
    $V_{m,n} \triangleq \{P\in\mathbb{R}^{m\times n}: P^\top P=\mathbf{1}_{n\times n}\}$</td>
    <td>This is the manifold of projection matrices. For optional
    argument $k>1$ this instantiates the product manifold
    $\mathcal{St}^{m\times n}\times\cdots\times\mathcal{St}^{m\times n}$
    of $k$ Stiefel manifolds.</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.manifolds import Stiefel
manifold = Stiefel(m,n)
</code></pre>
    </td>
</tr>

<tr class="topborder">
    <td>Product manifold<br />
    $\mathcal{M_1}\times \cdots \times \mathcal{M_l}$ for manifolds $\mathcal{M_1},...,\mathcal{M_l}$</td>
    <td>Product manifold of any of the manifolds listed above, e.g. $V_{m\times n}\times\mathbb{R}^{k\times l}$.</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.manifolds import Product, Stiefel, Euclidean
manifold = Product([Stiefel(m,n), Euclidean(k,l)])
</code></pre>
    </td>
</tr>

</table>


<h2>3. Solvers</h2>

<h3>List of Solvers</h3>

<table id="solvers">

<tr>
    <th>solver</th>
    <th>description</th>
    <th>required codelines</th>
</tr>

<tr>
    <td>TrustRegions</td>
    <td>Second-order method that approximates the objective function by
    a quadratic surface and then updates the current estimate based on
    that</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.solvers import TrustRegions
solver = TrustRegions()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>SteepestDescent</td>
    <td>Classical first-order steepest descent algorithm</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.solvers import SteepestDescent
solver = SteepestDescent()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>ConjugateGradient</td>
    <td>Classical first-order conjugate gradient descent algorithm</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.solvers import ConjugateGradient
solver = ConjugateGradient()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>NelderMead</td>
    <td>Derivative-free optimisation</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.solvers import NelderMead
solver = NelderMead()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>ParticleSwarm</td>
    <td>Derivative-free optimisation</td>
    <td>
<pre class="prettyprint"><code class="language-py">
from pymanopt.solvers import ParticleSwarm
solver = ParticleSwarm()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

</table>


<h2>Examples</h2>

<p>Several examples that demonstrate how to use Pymanopt can be found
<a href="https://github.com/pymanopt/pymanopt/tree/master/examples">here</a>.
Below are some examples that demonstrate certain use-cases that may be of
general interest and give an idea of what can be done with Pymanopt.</p>

<table>

<tr>
    <th>example</th>
    <th>notes</th>
    <th>file(s)</th>
</tr>

<tr id="example-optlog-1">
    <td>Log optimisation behaviour</td>
    <td>Demonstrates how to log and inspect the optimisation behaviour,
    i.e. how the cost function's value changes over iterations, which of
    the stopping criterions caused the solver to stop, etc.</td>
    <td>
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/optlog_example.py">
        optlog_example.py</a>
    </td>
</tr>

<tr id="example-productmanifold-1">
    <td>Linear regression as optimisation over the product manifold</td>
    <td>Optimises the weights $w$ and the offset $b$ in the linear regression
    problem $\min_{w,b} ||Y-w^\top X-b||_2^2$, for given label vector $Y$ and
    covariates matrix $X$ over the product manifold
    $\mathbb{R}^3 \times \mathbb{R}$ to demonstrate optimisation over
    product manifolds.</td>
    <td>
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/regression_offset_autograd.py">
        regression_offset_autograd.py</a> /
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/regression_offset_theano.py">
        regression_offset_theano.py</a>
    </td>
</tr>

<tr id="example-multiple-1">
    <td>Solve the same optimisation problems for several data instances</td>
    <td>Demonstrates how to solve the same optimisation problems for several
    data instances, i.e., in this case solving a regression problem for five
    different datasets.</td>
    <td>
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/linreg_multiple_autograd.py">
        linreg_multiple_autograd.py</a> /
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/linreg_multiple_theano.py">
        linreg_multiple_theano.py</a>
    </td>
</tr>

</table>


</body>

</html>
