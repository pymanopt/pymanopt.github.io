<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.1//EN" "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-2.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" dir="ltr" id="index">
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Pymanopt</title>

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

    <!-- mathjax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    <!-- code-prettify -->
    <script type="text/javascript" src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?lang=py"></script>

    <!-- skeleton.css -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/skeleton/2.0.4/skeleton.min.css" rel="stylesheet" type="text/css" />
    <link href="layout.css" rel="stylesheet" type="text/css" />
</head>

<body>

  <div class="section">
    <div class="container">

<h1>Pymanopt</h1>

<p>Pymanopt is a Python package for doing manifold optimization, that computes
gradients and Hessians automatically. It builds upon the MATLAB toolbox
<a href="http://manopt.org/">Manopt</a> but is otherwise independent of it.
Pymanopt aims to lower the barriers for users wishing to use state of the art
manifold optimization techniques, by relying on automatic differentiation for
computing gradients and Hessians, saving users time and saving them from
potential calculation and implementiation errors.</p>

<p>Pymanopt is modular and hence easy to use. All of the automatic
differentiation is done behind the scenes, so that the amount of setup the user
needs to do is minimal. Usually only the following steps are required:</p>
<ol>
    <li>Instantiation of a manifold $\mathcal{M}$ to optimise over</li>
    <li>Defininition of a cost function $f:\mathcal{M}\to \mathbb{R}$ to
      minimise</li>
    <li>Instantiation of a Pymanopt solver</li>
</ol>

<p>Experimenting with manifold optimisation is simple with Pymanopt. The
minimum working example below demonstrates this. The steps will be
discussed in more detail in the subsequent three sections. Please
refer to <a href="MoG.html">this example</a> for a <strong>cool
application</strong> of Riemannian Optimisation using Pymanopt for
Inference in MoG models!</p>

<p><strong>We encourage users and developers to report problems, request
features, ask for help, or leave general comments either on
<a href="https://github.com/pymanopt/pymanopt">github</a>,
<a href="https://gitter.im/pymanopt/pymanopt">gitter</a>, or via email to
one of the <a href="https://github.com/pymanopt/pymanopt/blob/master/MAINTAINERS">maintainers</a>.</strong>
Please refer to the <a href="doc/">dev documentation</a> and the
<a href="https://github.com/pymanopt/pymanopt/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a>
file if you wish to extend Pymanopt's functionality and/or contribute to
the project. Pymanopt is distributed under the open source
<a href="https://github.com/pymanopt/pymanopt/blob/master/LICENSE">3-clause BSD license</a>.</p>

<pre class="prettyprint"><code class="language-py">import autograd.numpy as np

from pymanopt.manifolds import Stiefel
from pymanopt import Problem
from pymanopt.solvers import SteepestDescent

# (1) Instantiate a manifold
manifold = Stiefel(5, 2)

# (2) Define the cost function (here using autograd.numpy)
def cost(X): return np.sum(X)

problem = Problem(manifold=manifold, cost=cost)

# (3) Instantiate a Pymanopt solver
solver = SteepestDescent()

# let Pymanopt do the rest
Xopt = solver.solve(problem)
print(Xopt)
</code></pre>


<h2>Installation</h2>
<h3>Dependencies</h3>
<p>Pymanopt is compatible with Python 2.7 and Python 3.3+, and depends on
NumPy and SciPy. Additionally, to use Pymanopt's built-in automatic
differentiation, which we strongly recommend, you need to setup your cost
functions using either
<a href="http://www.deeplearning.net/software/theano/">Theano</a> or
<a href="https://github.com/HIPS/autograd">Autograd</a>. If you're unfamiliar
with both packages and not sure which to go for, it's probably best to start
with Autograd. Autograd wraps thinly around NumPy, and is very simple to use,
particularly if you're already familiar with NumPy (see
<a href="#autograd-example">below</a>).</p>

<p>Instructions for installing NumPy, SciPy, and Theano on different operating
systems can be found
<a href="http://deeplearning.net/software/theano/install.html">here</a>,
for installing Autograd
<a href="https://github.com/HIPS/autograd#how-to-install">here</a>.</p>

<p>Pymanopt can be installed with the following command:</p>
<pre class="prettyprint"><code class="language-bash">pip install --user git+https://github.com/pymanopt/pymanopt.git
</code></pre>


<h2>Supported Manifolds</h2>

<p>The Pymanopt Manifold classes provide manifold specific routines like
computing the intrinsic mean of two points on the manifold or computing
the geodesic distance. The user will only need to instantiate the
correct manifold and need not worry about the internal workings. We plan
on implementing further manifolds as needed by the users. Developers
wanting to implement a new manifold for Pymanopt are referred to the
<a href="doc/index.html#module-pymanopt.manifolds.manifold">Manifold
abstract base class</a>.</p>

<table id="manifolds" class="u-full-width">

  <thead>
<tr>
    <th>Manifold</th>
    <th>Description</th>
    <th>Code</th>
</tr>
  </thead>

<tr>
    <td>Euclidean manifold<br />
    $\mathbb{R}^{m\times n}$</td>
    <td>Euclidean space of $m\times n$ matrices equipped with the
    Frobenius distance and trace inner product. Helpful for
    unconstrained problems or, when used as part of a product manifold,
    for unconstrained hyperparameters.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import Euclidean
manifold = Euclidean(m,n)
</code></pre>
    </td>
</tr>

<tr>
    <td>Grassmann manifold<br />
    $G_{m,n} \triangleq \{\operatorname{Span}\{M\}:M\in\mathbb{R}^{m\times n}\}$</td>
    <td>This is the manifold of $n$-dimensional subspaces of $\mathbb{R}^m$. For optional argument $k>1$ this instantiates the product $\mathcal{Gr}^{m\times n}\times\cdots\times\mathcal{Gr}^{m\times n}$ of $k$ Grassmann manifolds.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import Grassmann
manifold = Grassmann(m,n)
</code></pre>
    </td>
</tr>

<tr>
    <td>Oblique manifold<br />
    $O_{m,n} \triangleq \{M\in\mathbb{R}^{m\times n}: ||M_{:,j}||_2=1 \ \forall j\in\mathbb{N}_1^n\}$</td>
    <td>Manifold of matrices with unit-norm columns.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import Oblique
manifold = Oblique(m,n)
</code></pre>
    </td>
</tr>

<tr>
    <td>PositiveDefinite manifold<br />
    $\mathcal{PD}^n \triangleq \{M\in\mathbb{R}^{n\times n}: M \succ \mathbf{0}\}$</td>
    <td>Manifold of $n$-by-$n$ positive definite matrices.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import PositiveDefinite
manifold = PositiveDefinite(n)
</code></pre>
    </td>
</tr>

<tr>
    <td>Elliptope manifold<br />
    $\begin{align}
        E_k^n \triangleq \{M\in\mathbb{R}^{n\times n}: & M \succeq \mathbf{0}, \operatorname{rank}(M)=k,\\
                                                       & ||M_{i,:}||_2=1 \ \forall i\in\mathbb{N}_1^n\}
    \end{align}$</td>
    <td>Manifold of $n$-by-$n$ psd matrices of rank $k$ with unit diagonal elements.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import Elliptope
manifold = Elliptope(n,k)
</code></pre>
    </td>
</tr>

<tr>
    <td>PSDFixedRank manifold<br />
    $\operatorname{PSD}_k^n \triangleq \{M\in\mathbb{R}^{n\times n}: M \succeq \mathbf{0}, \operatorname{rank}(M)=k\}$</td>
    <td>Manifold of $n$-by-$n$ symmetric/hermitian positive semidefinite matrices of rank $k$.</td>
    <td>
<pre class="prettyprint"><code class="language-py"># real
from pymanopt.manifolds import PSDFixedRank
manifold = PSDFixedRank(n,k)

# complex
from pymanopt.manifolds import PSDFixedRankComplex
manifold = PSDFixedRankComplex(n,k)
</code></pre>
    </td>
</tr>

<tr>
    <td>Sphere manifold<br />
    $\mathcal{S}^{m\times n} \triangleq \{M\in\mathbb{R}^{m\times n}:||M||_F=1\}$</td>
    <td>$m\times n$ matrices of unit Frobenius norm, i.e., the unit
    Sphere for $n=1$ (the default value of this optional argument).</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import Sphere
manifold = Sphere(m)
</code></pre>
    </td>
</tr>

<tr>
    <td>Stiefel manifold<br />
    $V_{m,n} \triangleq \{P\in\mathbb{R}^{m\times n}: P^\top P=\mathbf{1}_{n\times n}\}$</td>
    <td>This is the manifold of projection matrices. For optional
    argument $k>1$ this instantiates the product manifold
    $\mathcal{St}^{m\times n}\times\cdots\times\mathcal{St}^{m\times n}$
    of $k$ Stiefel manifolds.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import Stiefel
manifold = Stiefel(m,n)
</code></pre>
    </td>
</tr>

<tr class="topborder">
    <td>Product manifold<br />
    $\mathcal{M_1}\times \cdots \times \mathcal{M_l}$ for manifolds $\mathcal{M_1},...,\mathcal{M_l}$</td>
    <td>Product manifold of any of the manifolds listed above, e.g. $V_{m\times n}\times\mathbb{R}^{k\times l}$.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.manifolds import Product, Stiefel, Euclidean
manifold = Product([Stiefel(m,n), Euclidean(k,l)])
</code></pre>
    </td>
</tr>

</table>


<h2>Cost Functions</h2>

<p>Both the manifold and cost function fully describe the Riemannian Optimisation
problem that is to be solved. They are combined into a Pymanopt Problem object that
is then passed to a Pymanopt solver.</p>
<pre class="prettyprint"><code class="language-py">from pymanopt import Problem
problem = Problem(manifold=manifold, cost=cost)
</code></pre>

<p>The cost function passed to Pymanopt should take a single input (a point on
the manifold), and return a scalar. You have three options for how to define
the cost function:</p>
<table id="cost-setup" class="u-full-width">
  <thead>
  <tr>
    <th></th>
    <th>Method</th>
    <th>Additional requirements</th>
  </tr>
  </thead>
  <tr>
    <td>(a)</td><td>Use Autograd</td><td>None</td>
  </tr>
  <tr>
    <td>(b)</td><td>Use Theano</td><td>None</td>
  </tr>
  <tr>
    <td>(c)</td><td>Use a regular Python function</td><td>Calculate and implement
    derivatives (gradient and Hessian) by hand.</td>
  </tr>
</table>

<p><strong>The first two options are recommended â€“ indeed, they are what makes
manifold optimization with Pymanopt so easy!</strong></p>

<h3>(a/b) Use Autograd or Theano</h3>

<p>Currently Pymanopt supports Theano and Autograd as autodiff backends. We
plan to implement support for
<a href="https://www.tensorflow.org">TensorFlow</a>.</p>

<h4 id="autograd-example">Setting up the cost function using Autograd</h4>
<p>If you already know how to use NumPy, then this approach will be easy. Just
import autograd.numpy and setup your cost as a Python function, using the
autograd numpy to perform the computation.</p>
<pre class="prettyprint"><code class="language-python">import autograd.numpy as np

def cost(X):
    return np.exp(-np.sum(X**2))

problem = Problem(manifold=manifold, cost=cost)
</code></pre>


<h4>Setting up the cost function using Theano</h4>
<p>This approach requires you to setup your cost as a Theano graph. A
tutorial on using Theano can be found
<a href="http://deeplearning.net/software/theano/tutorial/">here</a>.</p>
<pre class="prettyprint"><code class="language-python">import theano.tensor as T

X = T.matrix()
cost = T.exp(-T.sum(X**2))

problem = Problem(manifold=manifold, cost=cost, arg=X)
</code></pre>


<h3>(c) Use a regular Python function</h3>

<p>If you wish to use one of the derivative-free solvers (perhaps your cost
function is nowhere smooth), then this approach makes sense. If you want to use
a solver which requires derivatives (these solvers generally perform far better
than derivative-free methods) this approach enables you to calculate and
implement gradients and Hessians by hand.</p>

<p>Using Pymanopt in this way is similar to Manopt. You can refer to the Manopt
<a href="http://www.manopt.org/tutorial.html">tutorial</a> and
<a href="https://groups.google.com/forum/#!forum/manopttoolbox">forum</a> for
advice on calculating gradients/hessians by hand.
However, we would like to stress that there is <i>little or no advantage</i> to
doing things in this way. The gradients and Hessians calculated by Pymanopt
should be both correct and efficient.
</p>
<pre class="prettyprint"><code class="language-python">problem = Problem(manifold=manifold, cost=cost, egrad=egrad, ehess=ehess)
</code></pre>


<h2>Solvers</h2>

<p>The Pymanopt Solver classes provide the algorithms for optimisation.
Once a Pymanopt Problem object has been set up and a solver instantiated
the optimisation is run as follows:</p>
<pre class="prettyprint"><code class="language-py">xoptimal = solver.solve(problem)
</code></pre>
<p>The solvers' parameters are specified when instantiating the solver object.
The following parameters are shared by all solvers
(<code class="prettyprint, language-py">argumentname=defaultvalue</code>):</p>
<dl>
    <dt><code class="prettyprint, language-py">maxtime=1000</code></dt>
    <dd>Maximum time in seconds for the solver to run. You can set
    <code class="prettyprint, language-py">maxtime=float('inf')</code>
    for no time limit.</dd>

    <dt><code class="prettyprint, language-py">maxiter=1000</code></dt>
    <dd>Maximum number of iterations to run.</dd>

    <dt><code class="prettyprint, language-py">mingradnorm=1e-6</code></dt>
    <dd>Terminate optimisation if the norm of the gradient is below this.</dd>

    <dt><code class="prettyprint, language-py">minstepsize=1e-10</code></dt>
    <dd>Terminate optimisation if the stepsize is below this.</dd>

    <dt><code class="prettyprint, language-py">maxcostvals=5000</code></dt>
    <dd>Maximum number of allowed cost evaluations, especially important if
    cost evaluation is computationally expensive.</dd>

    <dt><code class="prettyprint, language-py">logverbosity=0</code></dt>
    <dd>Level of information logged by the solver while it operates, 0
    is silent, 2 is most information. If set to a non-zero value, the solver
    will return both the final point on the manifold as well as a dictionary
    that holds the log information, otherwise the solve routine only returns
    the final point, i.e.,
<pre class="prettyprint"><code class="language-py">xoptimal = solver.solve(problem, logverbosity=0)
xoptimal, optlog = solver.solve(problem, logverbosity=2)
</code></pre>
    There is also a minimalistic <a href="#example-optlog-1">example</a>.</dd>
</dl>

<p>Solvers may have individual parameters to adjust their behaviour. These
are documented in the respective source files. Developers wanting to implement
a new solver for Pymanopt are referred to the
<a href="doc/index.html#module-pymanopt.solvers.solver">Solver abstract
base class</a>.</p>

<table id="solvers" class="u-full-width">

  <thead>
<tr>
    <th>Solver</th>
    <th>Description</th>
    <th>Code</th>
</tr>
  </thead>

<tr>
    <td>TrustRegions</td>
    <td>Second-order method that approximates the objective function by
    a quadratic surface and then updates the current estimate based on
    that.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.solvers import TrustRegions
solver = TrustRegions()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>SteepestDescent</td>
    <td>Classical first-order steepest descent algorithm.
    By default uses a standard back tracking linesearch. To set its parameters
    you can instantiate the
    <a href="https://pymanopt.github.io/doc/index.html#pymanopt.solvers.linesearch.LineSearchBackTracking">LineSearchBackTracking</a>
    object with your desired parameters and hand it over to the
    SteepestDescent solver (<code class="language-py">solver = SteepestDescent(linesearch=LinesearchObject)</code>).
    It is also easy to implement your own linesearch class, just take
    <a href="https://github.com/pymanopt/pymanopt/blob/master/pymanopt/solvers/linesearch.py#L4">this code</a>
    as a starting point.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.solvers import SteepestDescent
solver = SteepestDescent()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>ConjugateGradient</td>
    <td>Classical first-order conjugate gradient descent algorithm.
    By default uses a standard adaptive linesearch. To set its parameters
    you can instantiate the
    <a href="https://pymanopt.github.io/doc/index.html#pymanopt.solvers.linesearch.LineSearchAdaptive">LineSearchAdaptive</a>
    object with your desired parameters and hand it over to the
    ConjugateGradient solver (<code class="language-py">solver = ConjugateGradient(linesearch=LinesearchObject)</code>).
    It is also easy to implement your own linesearch class, just take
    <a href="https://github.com/pymanopt/pymanopt/blob/master/pymanopt/solvers/linesearch.py#L81">this code</a>
    as a starting point.</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.solvers import ConjugateGradient
solver = ConjugateGradient()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>NelderMead</td>
    <td>Derivative-free optimisation</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.solvers import NelderMead
solver = NelderMead()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

<tr>
    <td>ParticleSwarm</td>
    <td>Derivative-free optimisation</td>
    <td>
<pre class="prettyprint"><code class="language-py">from pymanopt.solvers import ParticleSwarm
solver = ParticleSwarm()
Xopt = solver.solve(problem)
</code></pre>
    </td>
</tr>

</table>


<h2>Examples</h2>

<p>Several examples that demonstrate how to use Pymanopt can be found
<a href="https://github.com/pymanopt/pymanopt/tree/master/examples">here</a>.
Below are some examples that demonstrate certain use-cases that may be of
general interest and give an idea of what can be done with Pymanopt.</p>

<table class="u-full-width">

  <thead>
<tr>
    <th>Example</th>
    <th>Notes</th>
    <th>Links</th>
</tr>
  </thead>

<tr id="example-optlog-1">
    <td>Log optimisation behaviour</td>
    <td>Demonstrates how to log and inspect the optimisation behaviour,
    i.e. how the cost function's value changes over iterations, which of
    the stopping criterions caused the solver to stop, etc.</td>
    <td>
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/optlog_example.py">
        optlog_example.py</a>
    </td>
</tr>

<tr id="example-mog-1">
    <td>Riemannian Optimisation using Pymanopt for Inference in MoG models</td>
    <td>Given samples of a Mixture of Gaussians model, infer the parameters
    using manifold optimisation instead of expectation maximisation (EM).</td>
    <td>
        <a href="MoG.html">
        MoG.html</a> /
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/MoG.ipynb">
        MoG.ipynb</a>
    </td>
</tr>

<tr id="example-productmanifold-1">
    <td>Linear regression as optimisation over the product manifold</td>
    <td>Optimises the weights $w$ and the offset $b$ in the linear regression
    problem $\min_{w,b} ||Y-w^\top X-b||_2^2$, for given label vector $Y$ and
    covariates matrix $X$ over the product manifold
    $\mathbb{R}^3 \times \mathbb{R}$ to demonstrate optimisation over
    product manifolds.</td>
    <td>
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/regression_offset_autograd.py">
        regression_offset_autograd.py</a> /
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/regression_offset_theano.py">
        regression_offset_theano.py</a>
    </td>
</tr>

<tr id="example-multiple-1">
    <td>Solve the same optimisation problems for several data instances</td>
    <td>Demonstrates how to solve the same optimisation problems for several
    data instances, i.e., in this case solving a regression problem for five
    different datasets.</td>
    <td>
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/linreg_multiple_autograd.py">
        linreg_multiple_autograd.py</a> /
        <a href="https://github.com/pymanopt/pymanopt/blob/master/examples/linreg_multiple_theano.py">
        linreg_multiple_theano.py</a>
    </td>
</tr>

</table>

    </div>
  </div>

</body>

</html>
