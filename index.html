<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.1//EN" "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-2.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" dir="ltr" id="index">
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>pymanopt</title>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/styles/shCore.min.css" rel="stylesheet" type="text/css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/styles/shCoreDefault.min.css" rel="stylesheet" type="text/css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shCore.min.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushPython.min.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shAutoloader.min.js" type="text/javascript"></script>
    <script type="text/javascript">
        SyntaxHighlighter.all()
    </script>
</head>

<body>

<p><strong>This site is under construction. Please refer to this
<a href="https://github.com/pymanopt/pymanopt">github repository</a> in
the meanwhile.</strong></p>


<h1>pymanopt</h1>

<p>pymanopt is a python toolbox for manifold optimization that computes
gradients and hessians automatically. It builds upon the MATLAB package
<a href="http://manopt.org/">Manopt</a> but is otherwise independent of
it. Pymanopt aims to lower the barriers for users wishing to use state
of the art manifold optimization techniques even further, by relying on
automatic differentiation for computing gradients and hessians, saving
users time and saving them from potential calculation and
implementiation errors.</p>

<p>pymanopt is modular and hence easy to use. Usually only the following
steps are required:</p>
<ol>
    <li>defininition of the cost function $f:\mathcal{M}\to \mathbb{R}$
    to minimise</li>
    <li>instantiation of the respective manifold $\mathcal{M}$ to optimise
    over</li>
    <li>instantiation of a pymanopt solver</li>
</ol>
<p>If needed by the solver, pymanopt will compute the gradient and
hessian automatically.</p>

<pre class="brush: py">
import autograd.numpy as np

from pymanopt import Problem
from pymanopt.solvers import SteepestDescent
from pymanopt.manifolds import Stiefel

# (1) definition of the cost function (here using theano)
def cost(X): return np.sum(np.sum(X))

# (2) instantiation of the respective manifold
manifold = Stiefel(5, 2)

# (3) instantiation of a pymanopt solver
solver = SteepestDescent()

# let pymanopt do the rest
problem = Problem(man=manifold, ad_cost=cost, ad_arg=X)
Xopt = solver.solve(problem)
print(Xopt)
</pre>


<h2>Manifolds</h2>

<table>
<tr>
    <th>manifold</th>
    <th>description</th>
    <th>required codelines</th>
</tr>
<tr>
    <td>Euclidian manifold $\mathbb{R}^{m\times n}$</td>
    <td>Euclidian space of $m\times n$ matrices equipped with the
    Frobenius distance and trace inner product.</td>
    <td>
    <pre class="brush: py">
    from pymanopt.manifolds import Euclidian
    manifold = Euclidian(m,n)
    </pre>
    </td>
</tr>
<tr>
    <td>...</td>
    <td></td>
    <td></td>
</tr>
</table>

<h2>Solvers</h2>


</body>

</html>
